{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c519626",
   "metadata": {},
   "source": [
    "# Serving HuggingFace Transformer Models\n",
    "\n",
    "Out of the box, MLServer supports the deployment and serving of HuggingFace Transformer models with the following features:\n",
    "\n",
    "- Loading of Transformer Model artifacts from Hub.\n",
    "- Supports Optimized models using Optimum library\n",
    "\n",
    "In this example, we will showcase some of this features using an example model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e28a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required dependencies\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fed35e",
   "metadata": {},
   "source": [
    "## Serving\n",
    "\n",
    "Now that we have trained and serialised our model, we are ready to start serving it.\n",
    "For that, the initial step will be to set up a `model-settings.json` that instructs MLServer to load our artifact using the HuggingFace Inference Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6df62443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parallel_workers\": 0,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"pretrained_model\": \"distilgpt2\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6e8b2",
   "metadata": {},
   "source": [
    "Now that we have our config in-place, we can start the server by running `mlserver start .`. This needs to either be ran from the same directory where our config files are or pointing to the folder where they are.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```\n",
    "\n",
    "Since this command will start the server and block the terminal, waiting for requests, this will need to be ran in the background on a separate terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664c591",
   "metadata": {},
   "source": [
    "### Send test inference request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "759ad7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'transformer',\n",
       " 'model_version': None,\n",
       " 'id': '9b24304e-730f-4a98-bfde-8949851388a9',\n",
       " 'parameters': None,\n",
       " 'outputs': [{'name': 'output',\n",
       "   'shape': [1],\n",
       "   'datatype': 'BYTES',\n",
       "   'parameters': None,\n",
       "   'data': ['[{\"generated_text\": \"this is a test-case where you\\'re checking if someone\\'s going to have an encrypted file that they like to open, or whether their file has a hidden contents if their file is not opened. If it\\'s the same file, when all the\"}]']}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"args\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"this is a test\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbcf72",
   "metadata": {},
   "source": [
    "### Using Optimum Optimized Models\n",
    "\n",
    "We can also leverage the Optimum library that allows us to access quantized and optimized models. \n",
    "\n",
    "We can download pretrained optimized models from the hub if available by enabling the `optimum_model` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d185281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parallel_workers\": 0,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"pretrained_model\": \"distilgpt2\",\n",
    "            \"optimum_model\": true\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90925ef0",
   "metadata": {},
   "source": [
    "Once again, you are able to run the model using the MLServer CLI. As before this needs to either be ran from the same directory where our config files are or pointing to the folder where they are.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945db96",
   "metadata": {},
   "source": [
    "### Send Test Request to Optimum Optimized Model\n",
    "\n",
    "The request can now be sent using the same request structure but using optimized models for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39d8b438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'transformer',\n",
       " 'model_version': None,\n",
       " 'id': '296ea44e-7696-4584-af5a-148a7083b2e7',\n",
       " 'parameters': None,\n",
       " 'outputs': [{'name': 'output',\n",
       "   'shape': [1],\n",
       "   'datatype': 'BYTES',\n",
       "   'parameters': None,\n",
       "   'data': ['[{\"generated_text\": \"this is a test that allows us to define the value type, and a function is defined directly with these variables.\\\\n\\\\n\\\\nThe function is defined for a parameter with type\\\\nIn this example,\\\\nif you pass a message function like\\\\ntype\"}]']}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"args\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"this is a test\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c2a92",
   "metadata": {},
   "source": [
    "## Testing Supported Tasks\n",
    "\n",
    "We can support multiple other transformers other than just text generation, below includes examples for a few other tasks supported.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8106d",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a55092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parallel_workers\": 0,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"question-answering\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1e385",
   "metadata": {},
   "source": [
    "Once again, you are able to run the model using the MLServer CLI.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7aaf365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'gpt2-model',\n",
       " 'model_version': None,\n",
       " 'id': '204ad4e7-79ea-40b4-8efb-aed16dedf7ed',\n",
       " 'parameters': None,\n",
       " 'outputs': [{'name': 'output',\n",
       "   'shape': [1],\n",
       "   'datatype': 'BYTES',\n",
       "   'parameters': None,\n",
       "   'data': ['{\"score\": 0.9869922995567322, \"start\": 12, \"end\": 18, \"answer\": \"Seldon\"}']}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"question\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"what is your name?\"],\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"context\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"Hello, I am Seldon, how is it going\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e6dd1",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a49ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parallel_workers\": 0,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-classification\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b4eaf",
   "metadata": {},
   "source": [
    "Once again, you are able to run the model using the MLServer CLI.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd253078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'transformer',\n",
       " 'model_version': None,\n",
       " 'id': '463ceddb-f426-4815-9c46-9fa9fc5272b1',\n",
       " 'parameters': None,\n",
       " 'outputs': [{'name': 'output',\n",
       "   'shape': [1],\n",
       "   'datatype': 'BYTES',\n",
       "   'parameters': None,\n",
       "   'data': ['[{\"label\": \"NEGATIVE\", \"score\": 0.9996137022972107}]']}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"args\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"This is terrible!\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b09e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
