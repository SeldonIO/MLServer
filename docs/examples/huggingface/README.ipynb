{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c519626",
   "metadata": {},
   "source": [
    "# Serving HuggingFace Transformer Models\n",
    "\n",
    "Out of the box, MLServer supports the deployment and serving of HuggingFace Transformer models with the following features:\n",
    "\n",
    "- Loading of Transformer Model artifacts from the Hugging Face Hub.\n",
    "- Model quantization & optimization using the Hugging Face Optimum library\n",
    "- Request batching for GPU optimization (via adaptive batching and request batching)\n",
    "\n",
    "In this example, we will showcase some of this features using an example model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b2588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "# Import required dependencies\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fed35e",
   "metadata": {},
   "source": [
    "## Serving\n",
    "\n",
    "Since we're using a pretrained model, we can skip straight to serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205410c",
   "metadata": {},
   "source": [
    "### `model-settings.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df62443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"pretrained_model\": \"distilgpt2\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6e8b2",
   "metadata": {},
   "source": [
    "Now that we have our config in-place, we can start the server by running `mlserver start .`. This needs to either be ran from the same directory where our config files are or pointing to the folder where they are.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```\n",
    "\n",
    "Since this command will start the server and block the terminal, waiting for requests, this will need to be ran in the background on a separate terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664c591",
   "metadata": {},
   "source": [
    "### Send test inference request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ad7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"args\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"this is a test\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "requests.post(\n",
    "    \"http://localhost:8080/v2/models/transformer/infer\", json=inference_request\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbcf72",
   "metadata": {},
   "source": [
    "### Using Optimum Optimized Models\n",
    "\n",
    "We can also leverage the Optimum library that allows us to access quantized and optimized models. \n",
    "\n",
    "We can download pretrained optimized models from the hub if available by enabling the `optimum_model` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d185281",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"pretrained_model\": \"distilgpt2\",\n",
    "            \"optimum_model\": true\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90925ef0",
   "metadata": {},
   "source": [
    "Once again, you are able to run the model using the MLServer CLI. As before this needs to either be ran from the same directory where our config files are or pointing to the folder where they are.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6945db96",
   "metadata": {},
   "source": [
    "### Send Test Request to Optimum Optimized Model\n",
    "\n",
    "The request can now be sent using the same request structure but using optimized models for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"args\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"this is a test\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "requests.post(\n",
    "    \"http://localhost:8080/v2/models/transformer/infer\", json=inference_request\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88a78f",
   "metadata": {},
   "source": [
    "## Testing Supported Tasks\n",
    "\n",
    "We can support multiple other transformers other than just text generation, below includes examples for a few other tasks supported.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee06c8",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"question-answering\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf39e0",
   "metadata": {},
   "source": [
    "Once again, you are able to run the model using the MLServer CLI.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aaf365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"question\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"what is your name?\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"context\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"Hello, I am Seldon, how is it going\"],\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "requests.post(\n",
    "    \"http://localhost:8080/v2/models/transformer/infer\", json=inference_request\n",
    ").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa51356",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-classification\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed336525",
   "metadata": {},
   "source": [
    "Once again, you are able to run the model using the MLServer CLI.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f704413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"args\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"This is terrible!\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "requests.post(\n",
    "    \"http://localhost:8080/v2/models/transformer/infer\", json=inference_request\n",
    ").json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9277268a",
   "metadata": {},
   "source": [
    "### Masked Language Modeling\n",
    "\n",
    "We can also serve a masked language model. In the following example, we also build the `huggingface` runtime with the `-E japanese` flag to enable support for Japanese tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11117091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "  \"name\": \"transformer\",\n",
    "  \"implementation\": \"mlserver_huggingface.runtime.HuggingFaceRuntime\",\n",
    "  \"parameters\": {\n",
    "    \"extra\": {\n",
    "      \"task\": \"fill-mask\",\n",
    "      \"pretrained_model\": \"izumi-lab/bert-small-japanese\",\n",
    "      \"pretrained_tokenizer\": \"izumi-lab/bert-small-japanese\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3c359e2",
   "metadata": {},
   "source": [
    "Using the shell to start mlserver like so,\n",
    "```shell\n",
    "mlserver start .\n",
    "```\n",
    "we can pass inferences like this. Note the `[MASK]` token. The mask token can be different for different models, so check the HuggingFace model config for special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8739303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{'model_name': 'transformer', 'id': '9e966d8d-b43d-4ab4-8d47-90e367196233', 'parameters': {}, 'outputs': [{'name': 'output', 'shape': [5, 1], 'datatype': 'BYTES', 'parameters': {'content_type': 'hg_jsonlist'}, 'data': ['{\"score\": 0.3277095854282379, \"token\": 11819, \"token_str\": \"\\\\u3042\\\\u308b\", \"sequence\": \"\\\\u5b9f\\\\u969b \\\\u306b \\\\u7a7a \\\\u304c \\\\u3042\\\\u308b \\\\u306e \\\\u304b?\"}', '{\"score\": 0.10271108895540237, \"token\": 14656, \"token_str\": \"\\\\u898b\\\\u3048\\\\u308b\", \"sequence\": \"\\\\u5b9f\\\\u969b \\\\u306b \\\\u7a7a \\\\u304c \\\\u898b\\\\u3048\\\\u308b \\\\u306e \\\\u304b?\"}', '{\"score\": 0.08325661718845367, \"token\": 11835, \"token_str\": \"\\\\u306a\\\\u3044\", \"sequence\": \"\\\\u5b9f\\\\u969b \\\\u306b \\\\u7a7a \\\\u304c \\\\u306a\\\\u3044 \\\\u306e \\\\u304b?\"}', '{\"score\": 0.036131054162979126, \"token\": 18413, \"token_str\": \"\\\\u6b63\\\\u3057\\\\u3044\", \"sequence\": \"\\\\u5b9f\\\\u969b \\\\u306b \\\\u7a7a \\\\u304c \\\\u6b63\\\\u3057\\\\u3044 \\\\u306e \\\\u304b?\"}', '{\"score\": 0.029351236298680305, \"token\": 11820, \"token_str\": \"\\\\u3044\\\\u308b\", \"sequence\": \"\\\\u5b9f\\\\u969b \\\\u306b \\\\u7a7a \\\\u304c \\\\u3044\\\\u308b \\\\u306e \\\\u304b?\"}']}]}\n",
      "Data:\n",
      "{'score': 0.3277095854282379, 'token': 11819, 'token_str': 'ある', 'sequence': '実際 に 空 が ある の か?'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Test sentence: Is the sky really [MASK]?\n",
    "test_sentence = \"実際に空が[MASK]のか？\"\n",
    "# [MASK] = visible\n",
    "expected_output = \"見える\"\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"args\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\" : [test_sentence]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "response = requests.post(\n",
    "    \"http://localhost:8080/v2/models/transformer/infer\", json=inference_request\n",
    ").json()\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print(\"Data:\")\n",
    "print(json.loads(response['outputs'][0]['data'][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "545307aa",
   "metadata": {},
   "source": [
    "### Masked Language Modeling\n",
    "\n",
    "We can also serve a masked language model. In the following example, we also build the `huggingface` runtime with the `-E japanese` flag to enable support for Japanese tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "  \"name\": \"model\",\n",
    "  \"implementation\": \"mlserver_huggingface.runtime.HuggingFaceRuntime\",\n",
    "  \"parameters\": {\n",
    "    \"extra\": {\n",
    "      \"task\": \"fill-mask\",\n",
    "      \"pretrained_model\": \"cl-tohoku/bert-base-japanese\",\n",
    "      \"pretrained_tokenizer\": \"cl-tohoku/bert-base-japanese\"\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4d34159",
   "metadata": {},
   "source": [
    "Using the shell to start mlserver like so,\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```\n",
    "we can pass inferences like this. Note the `[MASK]` token. The mask token can be different for different models, so check the HuggingFace model config for special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserver_huggingface import HuggingFaceRuntime\n",
    "# Test sentence: Is the sky really [MASK]?\n",
    "test_sentence = \"実際に空が[MASK]のか？\"\n",
    "# [MASK] = visible\n",
    "expected_output = \"見える\"\n",
    "\n",
    "\n",
    "inference_request = HuggingfaceRequestCodec.encode_request(\n",
    "    {\"inputs\": [test_sentence]},\n",
    "    use_bytes=False,\n",
    ")\n",
    "requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6655d9",
   "metadata": {},
   "source": [
    "## GPU Acceleration\n",
    "\n",
    "We can also evaluate GPU acceleration, we can test the speed on CPU vs GPU using the following parameters\n",
    "\n",
    "### Testing with CPU\n",
    "\n",
    "We first test the time taken with the device=-1 which configures CPU by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827472eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"max_batch_size\": 128,\n",
    "    \"max_batch_time\": 1,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"device\": -1\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb4d12",
   "metadata": {},
   "source": [
    "Once again, you are able to run the model using the MLServer CLI.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888501c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"text_inputs\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"This is a generation for the work\" for i in range(512)],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Benchmark time\n",
    "import time\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "requests.post(\n",
    "    \"http://localhost:8080/v2/models/transformer/infer\", json=inference_request\n",
    ")\n",
    "\n",
    "print(f\"Elapsed time: {time.monotonic() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92756662",
   "metadata": {},
   "source": [
    "We can see that it takes 81 seconds which is 8 times longer than the gpu example below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e208c",
   "metadata": {},
   "source": [
    "### Testing with GPU\n",
    "\n",
    "IMPORTANT: Running the code below requries having a machine with GPU configured correctly to work for Tensorflow/Pytorch.\n",
    "    \n",
    "Now we'll run the benchmark with GPU configured, which we can do by setting `device=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"device\": 0\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"text_inputs\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"This is a generation for the work\" for i in range(512)],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Benchmark time\n",
    "import time\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "requests.post(\n",
    "    \"http://localhost:8080/v2/models/transformer/infer\", json=inference_request\n",
    ")\n",
    "\n",
    "print(f\"Elapsed time: {time.monotonic() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929108c",
   "metadata": {},
   "source": [
    "We can see that the elapsed time is 8 times less than the CPU version!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d790b37",
   "metadata": {},
   "source": [
    "### Adaptive Batching with GPU\n",
    "\n",
    "We can also see how the adaptive batching capabilities can allow for GPU acceleration by grouping multiple incoming requests so they get processed in GPU batch.\n",
    "\n",
    "In our case we can enable adaptive batching with the `max_batch_size` which in our case we will set it ot 128.\n",
    "\n",
    "We will also configure `max_batch_time` which specifies` the maximum amount of time the MLServer orchestrator will wait before sending for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"max_batch_size\": 128,\n",
    "    \"max_batch_time\": 1,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"pretrained_model\": \"distilgpt2\",\n",
    "            \"device\": 0\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063ccb6",
   "metadata": {},
   "source": [
    "In order to achieve the throughput required of 50 requests per second, we will use the tool `vegeta` which performs load testing.\n",
    "\n",
    "We can now see that we are able to see that the requests are batched and we receive 100% success eventhough the requests are sent one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a787909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "jq -ncM '{\"method\": \"POST\", \"header\": {\"Content-Type\": [\"application/json\"] }, \"url\": \"http://localhost:8080/v2/models/transformer/infer\", \"body\": \"{\\\"inputs\\\":[{\\\"name\\\":\\\"text_inputs\\\",\\\"shape\\\":[1],\\\"datatype\\\":\\\"BYTES\\\",\\\"data\\\":[\\\"test\\\"]}]}\" | @base64 }' \\\n",
    "          | vegeta \\\n",
    "                -cpus=\"2\" \\\n",
    "                attack \\\n",
    "                -duration=\"3s\" \\\n",
    "                -rate=\"50\" \\\n",
    "                -format=json \\\n",
    "          | vegeta \\\n",
    "                report \\\n",
    "                -type=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcb458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "mlserver_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
