[tool.poetry]
name = "mlserver"
version = "1.5.0"
description = "MLServer"
authors = ["Seldon Technologies Ltd. <hello@seldon.io>"]
license = "Apache-2.0"
readme = "README.md"
classifiers = [
    "Operating System :: POSIX",
    "Operating System :: MacOS"
]
include =  ["mlserver/rest/openapi/*.json"]

[tool.poetry.scripts]
mlserver = 'mlserver.cli:main'

[tool.black]
exclude = '''
(
  mlserver\.egg-info
  | \.mypy_cache
  | \.git
  | \.tox
  | dist
  | venv
)
'''

[tool.mypy]
ignore_missing_imports = true
plugins = "pydantic.mypy"
exclude = [
  'mlserver\.egg-info',
  '\.mypy_cache',
  '\.git',
  '\.tox',
  'dist',
  'venv',
]

[tool.pytest.ini_options]
asyncio_mode = "auto"
addopts = "--import-mode=importlib"

[tool.poetry.dependencies]
python = ">=3.9,<3.12"
click = "*"
fastapi = ">=0.88.0,!=0.89.0,<=0.110.0"
python-dotenv = "*"
grpcio = "*"
numpy = "*"
pandas = "*"
protobuf = "*"
uvicorn = "*"
starlette-exporter = "*"
py-grpc-prometheus = "*"
aiokafka = "*"
# add a min version to tritonclient due to https://github.com/triton-inference-server/server/issues/6246
tritonclient = {version = ">=2.42", extras = ["http"]}
geventhttpclient = "*"
gevent = "*"
aiofiles = "*"
orjson = "*"
uvloop = {version = "*", markers = "sys_platform != 'win32' and (sys_platform != 'cygwin' and platform_python_implementation != 'PyPy')"}
pydantic = "<2.0.0"
python-multipart = "*"

## The importlib-resources backport is required to use some
## functionality added in Python 3.10
## https://setuptools.pypa.io/en/latest/userguide/datafiles.html#accessing-data-files-at-runtime
importlib-resources = "^5.12.0"
opentelemetry-sdk = "^1.22.0"
opentelemetry-instrumentation-fastapi = ">=0.43b0"
opentelemetry-instrumentation-grpc = ">=0.43b0"
opentelemetry-exporter-otlp-proto-grpc = "^1.22.0"

[tool.poetry.group.dev.dependencies]
datamodel-code-generator = "0.25.4"
grpcio-tools = "1.48.1"
pytest = "7.4.4"
pytest-asyncio = "0.21.1"
pytest-mock = "3.12.0"
pytest-cases = "3.8.2"
tox = "4.6.3"
docker = "7.0.0"
aiohttp = "3.9.2"
aiohttp-retry = "2.8.3"
## Used for FastAPI Async testing
httpx = "0.26.0"
kafka-python = "2.0.2"
tenacity = "8.2.3"
pyyaml = "6.0.1"
conda-pack = "0.7.1"
flake8 = "7.0.0"
flake8-black = "0.3.6"
mypy = "1.8.0"
mypy-protobuf = "3.1.0"
types-protobuf = "4.24.0.20240129"
types-orjson = "3.6.2"
types-aiofiles = "23.2.0.20240106"
types-requests = "2.28.11.5"
black = "24.2.0"
pip-licenses = "4.3.4"
pytest-xdist = "3.5.0"
filelock = "^3.13.1"

[tool.poetry.group.docker.dependencies]
protobuf = "3.20.3"
tensorflow = "<2.15"

[tool.poetry.group.all-runtimes]
optional = true

[tool.poetry.group.all-runtimes.dependencies]
mlserver-sklearn = {path = "./runtimes/sklearn", develop = true}
mlserver-xgboost = {path = "./runtimes/xgboost", develop = true}
mlserver-lightgbm = {path = "./runtimes/lightgbm", develop = true}
mlserver-mlflow = {path = "./runtimes/mlflow", develop = true}
mlserver-huggingface = {path = "./runtimes/huggingface", develop = true}
mlserver-alibi-explain = {path = "./runtimes/alibi-explain", develop = true}
mlserver-alibi-detect = {path = "./runtimes/alibi-detect", develop = true}

[tool.poetry.group.all-runtimes-dev]
optional = true

[tool.poetry.group.all-runtimes-dev.dependencies]
## Dev dependencies from Alibi-Explain
requests-mock = "~1.11.0"
types-requests = "~2.28.11.5"

## Dev dependencies from MLflow
torch = "2.2.1"
pytorch-lightning = "2.2.0.post0"
torchmetrics = "1.3.1"
torchvision = "0.17.1"
mlflow = "2.10.2"

## Dev dependencies from HuggingFace
transformers = ">=4.30,<5.0"

[tool.poetry.group.docs]
optional = true

[tool.poetry.group.docs.dependencies]
Sphinx = "6.2.1"
sphinx_material = "0.0.36"
readthedocs-sphinx-search = "0.3.2"
myst-parser = "2.0.0"
sphinxcontrib-bibtex = "2.5.0"
autodoc_pydantic = "1.9.0"
sphinx-click = "5.1.0"
sphinx_design = "0.4.1"
sphinx-autobuild = "2021.3.14"
sphinx-copybutton = "^0.5.2"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
