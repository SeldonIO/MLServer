{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ambient-prize",
   "metadata": {},
   "source": [
    "# Content type decoding\n",
    "\n",
    "MLServer extends the V2 inference protocol by adding support for a `content_type` annotation.\n",
    "This annotation can be provided either through the model metadata `tags`, or through the input `parameters`.\n",
    "By leveraging the `content_type` annotation, we can provide the necessary information to MLServer so that it can _decode_ the input payload from the \"wire\" V2 protocol to something meaningful to the model / user (e.g. a NumPy array).\n",
    "\n",
    "This example will walk you through some examples which illustrate how this works, and how it can be extended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-haiti",
   "metadata": {},
   "source": [
    "## Echo Inference Runtime\n",
    "\n",
    "To start with, we will write a _dummy_ runtime which just prints the input, the _decoded_ input and returns it.\n",
    "This will serve as a testbed to showcase how the `content_type` support works.\n",
    "\n",
    "Later on, we will extend this runtime by adding custom _codecs_ that will decode our V2 payload to custom types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "inside-mouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting runtime.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runtime.py\n",
    "import json\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput\n",
    "\n",
    "class EchoRuntime(MLModel):\n",
    "    async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n",
    "        outputs = []\n",
    "        for request_input in payload.inputs:\n",
    "            decoded_input = self.decode(request_input)\n",
    "            print(f\"------ Encoded Input ({request_input.name}) ------\")\n",
    "            print(json.dumps(request_input.dict(), indent=2))\n",
    "            print(f\"------ Decoded input ({request_input.name}) ------\")\n",
    "            print(decoded_input)\n",
    "            \n",
    "            outputs.append(\n",
    "                ResponseOutput(\n",
    "                    name=request_input.name,\n",
    "                    datatype=request_input.datatype,\n",
    "                    shape=request_input.shape,\n",
    "                    data=request_input.data\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return InferenceResponse(model_name=self.name, outputs=outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-individual",
   "metadata": {},
   "source": [
    "As you can see above, this runtime will decode the incoming payloads by calling the `self.decode()` helper method.\n",
    "This method will check what's the right content type for each input in the following order:\n",
    "\n",
    "1. Is there any content type defined in the `inputs[].parameters.content_type` field within the **request payload**?\n",
    "2. Is there any content type defined in the `inputs[].tags.content_type` field within the **model metadata**?\n",
    "3. Is there any default content type that should be assumed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-missouri",
   "metadata": {},
   "source": [
    "### Model Settings\n",
    "\n",
    "In order to enable this runtime, we will also create a `model-settings.json` file.\n",
    "This file should be present (or accessible from) in the folder where we run `mlserver start .`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "veterinary-munich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile model-settings.json\n",
    "\n",
    "{\n",
    "    \"name\": \"content-type-example\",\n",
    "    \"implementation\": \"runtime.EchoRuntime\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-velvet",
   "metadata": {},
   "source": [
    "## Request Input Parameters\n",
    "\n",
    "Our initial step will be to decide the content type based on the incoming `inputs[].parameters` field.\n",
    "For this, we will start our MLServer in the background (e.g. running `mlserver start .`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "reliable-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"parameters-np\",\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"shape\": [2, 2],\n",
    "            \"data\": [1, 2, 3, 4],\n",
    "            \"parameters\": {\n",
    "                \"content_type\": \"np\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"parameters-str\",\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"shape\": [11],\n",
    "            \"data\": \"hello world ðŸ˜\",\n",
    "            \"parameters\": {\n",
    "                \"content_type\": \"str\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8080/v2/models/content-type-example/infer\",\n",
    "    json=payload\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-glass",
   "metadata": {},
   "source": [
    "## Model Metadata\n",
    "\n",
    "Our next step will be to define the expected content type through the model metadata.\n",
    "This can be done by extending the `model-settings.json` file, and adding a section on inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "female-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile model-settings.json\n",
    "\n",
    "{\n",
    "    \"name\": \"content-type-example\",\n",
    "    \"implementation\": \"runtime.EchoRuntime\",\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"metadata-np\",\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"shape\": [2, 2],\n",
    "            \"tags\": {\n",
    "                \"content_type\": \"np\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata-str\",\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"shape\": [11],\n",
    "            \"tags\": {\n",
    "                \"content_type\": \"str\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-harbor",
   "metadata": {},
   "source": [
    "After adding this metadata, we will re-start MLServer (e.g. `mlserver start .`) and we will send a new request without any explicit `parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "north-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"metadata-np\",\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"shape\": [2, 2],\n",
    "            \"data\": [1, 2, 3, 4],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"metadata-str\",\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"shape\": [11],\n",
    "            \"data\": \"hello world ðŸ˜\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8080/v2/models/content-type-example/infer\",\n",
    "    json=payload\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-organization",
   "metadata": {},
   "source": [
    "As you should be able to see in the server logs, MLServer will cross-reference the input names against the model metadata to find the right content type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-lewis",
   "metadata": {},
   "source": [
    "## Custom Codecs\n",
    "\n",
    "There may be cases where a custom inference runtime may need to encode / decode to custom datatypes.\n",
    "As an example, we can think of computer vision models which may only operate with `pillow` image objects.\n",
    "\n",
    "In these scenarios, it's possible to extend the `Codec` interface to write our custom encoding logic.\n",
    "A `Codec`, is simply an object which defines a `decode()` and `encode()` methods.\n",
    "To illustrate how this would work, we will extend our custom runtime to add a custom `PillowCodec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "lucky-great",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting runtime.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile runtime.py\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.types import (\n",
    "    InferenceRequest,\n",
    "    InferenceResponse, \n",
    "    RequestInput, \n",
    "    ResponseOutput\n",
    ")\n",
    "from mlserver.codecs import NumpyCodec, _codec_registry as codecs\n",
    "\n",
    "class PillowCodec(NumpyCodec):\n",
    "    ContentType = \"img\"\n",
    "    DefaultMode = \"L\"\n",
    "    \n",
    "    def encode(self, name: str, payload: Image) -> ResponseOutput:\n",
    "        byte_array = io.BytesIO()\n",
    "        payload.save(byte_array, mode=self.DefaultMode)\n",
    "        \n",
    "        return ResponseOutput(\n",
    "            name=name,\n",
    "            shape=payload.size,\n",
    "            datatype=\"BYTES\",\n",
    "            data=byte_array.getvalue()\n",
    "        )\n",
    "    \n",
    "    def decode(self, request_input: RequestInput) -> Image:\n",
    "        if request_input.datatype != \"BYTES\":\n",
    "            # If not bytes, assume it's an array\n",
    "            image_array = super().decode(request_input)\n",
    "            return Image.fromarray(image_array, mode=self.DefaultMode)\n",
    "        \n",
    "        encoded = request_input.data.__root__\n",
    "        if isinstance(encoded, str):\n",
    "            encoded = encoded.encode()\n",
    "\n",
    "        return Image.frombytes(\n",
    "            mode=self.DefaultMode,\n",
    "            size=request_input.shape,\n",
    "            data=encoded\n",
    "        )\n",
    "    \n",
    "# Register our new codec so that it's used for payloads with `img` content type\n",
    "codecs.register(content_type=PillowCodec.ContentType, codec=PillowCodec())\n",
    "\n",
    "class EchoRuntime(MLModel):\n",
    "    async def predict(self, payload: InferenceRequest) -> InferenceResponse:\n",
    "        outputs = []\n",
    "        for request_input in payload.inputs:\n",
    "            decoded_input = self.decode(request_input)\n",
    "            print(f\"------ Encoded Input ({request_input.name}) ------\")\n",
    "            print(json.dumps(request_input.dict(), indent=2))\n",
    "            print(f\"------ Decoded input ({request_input.name}) ------\")\n",
    "            print(decoded_input)\n",
    "            \n",
    "            outputs.append(\n",
    "                ResponseOutput(\n",
    "                    name=request_input.name,\n",
    "                    datatype=request_input.datatype,\n",
    "                    shape=request_input.shape,\n",
    "                    data=request_input.data\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return InferenceResponse(model_name=self.name, outputs=outputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-passion",
   "metadata": {},
   "source": [
    "We should now be able to restart our instance of MLServer (i.e. with the `mlserver start .` command), to send a few test requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "touched-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"image-int32\",\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"shape\": [8, 8],\n",
    "            \"data\": [\n",
    "                1, 0, 1, 0, 1, 0, 1, 0,\n",
    "                1, 0, 1, 0, 1, 0, 1, 0,\n",
    "                1, 0, 1, 0, 1, 0, 1, 0,\n",
    "                1, 0, 1, 0, 1, 0, 1, 0,\n",
    "                1, 0, 1, 0, 1, 0, 1, 0,\n",
    "                1, 0, 1, 0, 1, 0, 1, 0,\n",
    "                1, 0, 1, 0, 1, 0, 1, 0,\n",
    "                1, 0, 1, 0, 1, 0, 1, 0\n",
    "            ],\n",
    "            \"parameters\": {\n",
    "                \"content_type\": \"img\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"image-bytes\",\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"shape\": [8, 8],\n",
    "            \"data\": (\n",
    "                \"10101010\"\n",
    "                \"10101010\"\n",
    "                \"10101010\"\n",
    "                \"10101010\"\n",
    "                \"10101010\"\n",
    "                \"10101010\"\n",
    "                \"10101010\"\n",
    "                \"10101010\"\n",
    "            ),\n",
    "            \"parameters\": {\n",
    "                \"content_type\": \"img\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8080/v2/models/content-type-example/infer\",\n",
    "    json=payload\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-mauritius",
   "metadata": {},
   "source": [
    "As you should be able to see in the MLServer logs, the server is now able to decode the payload into a Pillow image.\n",
    "This example also illustrates how `Codec` objects can be compatible with multiple `datatype` values (e.g. tensor and `BYTES` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-language",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
